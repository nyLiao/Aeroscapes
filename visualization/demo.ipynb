{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigating Class Imbalance in Aerial Images Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:9000px;\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "}\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    ".p-Widget.jp-RenderedImage.jp-mod-trusted.jp-OutputArea-output { \n",
    "    display: table-cell;\n",
    "    text-align: center; \n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import scipy.ndimage\n",
    "import imgviz\n",
    "from sankey import sankey\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "cwd = os.getcwd()\n",
    "pwd = cwd[:cwd.rfind('/')]\n",
    "sys.path.append(pwd)\n",
    "from model import *\n",
    "from logger import Logger, ModelLogger\n",
    "from dataloader import DatasetTrainClean, DatasetClean, DatasetVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing aerial images needs to figure out the features in given outdoor scenes including stuff and things. In this project, we specifically look into a very ubiquitous issue in the context of aerial image semantic segmentation, that is, **class imbalance**. \n",
    "\n",
    "As their names suggest, aerial images are shot from the sky. So things such as humans and cars in the image become much smaller than everyday photos and thence hard to identify. Such extremely imbalanced distribution is difficult for training neural networks, and the model tends to overlook parts or entire things in images and regards them as unimportant stuffs, which is considered as false negatives. \n",
    "This phenomenon is harmful in many realistic applications of semantic segmentation. For example, drones based on the misclassified output may ignore humans or obstacles in the way, and cause bodily injury or property damage. Hence, mitigating the class imbalance issue is an urgent and important task for aerial image segmentation.\n",
    "\n",
    "<!-- Several techniques have been proposed to address the imbalance issue, which can be mainly divided into two categories. Re-sampling methods modifies the sampling process in the training stage to achieve a less biased instance distribution that are more suitable for the neural networks to learn. Another category is the cost-sensitive approach, which aims to fit the cost of classifying unevenly for classes of different frequencies.  -->\n",
    "Based on the nature of the aerial dataset, we choose to address the imbalance problem mainly with loss functions designed to differentiate the cost of learning classes. We choose five losses which cover a broad range of designs and are commonly used in classification and segmentation tasks, they are: Cross Entropy, Focal Loss, Dice Loss, IoU Loss, and Tversky Loss. Cross Entropy and Focal Loss are distribution-based loss, while Dice Loss, IoU Loss, and Tversky Loss are region-based loss. Cross Entropy Loss is the baseline that does not consider class imbalance at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issave = True\n",
    "data_path = '../data/'\n",
    "trn_dataset = DatasetTrainClean(data_path)\n",
    "val_dataset = DatasetVal(data_path)\n",
    "cln_dataset = DatasetClean(data_path)\n",
    "trn_dataloader = torch.utils.data.DataLoader(trn_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=1)\n",
    "cln_dataloader = torch.utils.data.DataLoader(cln_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "cln_dataloader2 = torch.utils.data.DataLoader(cln_dataset, batch_size=2, shuffle=False, num_workers=1)\n",
    "labs = ['bckgrnd', 'person', 'bike', 'car', 'drone', 'boat', 'animal', 'obstacle', 'constrn', 'plant', 'road', 'sky']\n",
    "cmap = np.array([\n",
    "        (  0,   0,   0),  #  Background\n",
    "        (255, 127,  14),  #  Person\n",
    "        (  0, 128,   0),  #  Bike\n",
    "        (152,  78, 163),  #  Car\n",
    "        (128,   0,   0),  #  Drone\n",
    "        (  0,   0, 128),  #  Boat\n",
    "        (192,   0, 128),  #  Animal\n",
    "        (192,   0,   0),  #  Obstacle\n",
    "        (192, 128,   0),  #  Construction\n",
    "        (  0,  64,   0),  #  Plant\n",
    "        (128, 128,   0),  #  Road\n",
    "        (  0, 128, 128)   #  Sky\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we utilize the AeroScapes dataset published in 2018. The dataset comprises of more than 3,000 images shot by a drone at a relatively low altitude. It contains 11 classes, which mainly belongs to 2 categories: stuff such as vegetation, roads, and sky; and things such as person, bikes, and cars. \n",
    "\n",
    "### Example Images\n",
    "\n",
    "Firstly, we here demonstrate a set of example images from the dataset. From left to right, they are raw images, label maps, and overlayed labels. It can be intuitively observed from the examples that, the things instances are relatively small in most images, only accounting for a small fraction of the total area, which explains why the pixel distribution is imbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rgb(img, label_img):\n",
    "    img_base = imgviz.color.rgb2gray(img)\n",
    "    labelviz = imgviz.label2rgb(\n",
    "            label=label_img, image=img_base, \n",
    "            label_names=labs, colormap=cmap, font_size=25, loc=\"centroid\")\n",
    "    return labelviz\n",
    "\n",
    "text_kwargs = dict(ha='center', va='center', fontsize=6, color='w')\n",
    "for i, idx in enumerate([0, 1100]):\n",
    "    img, label_img = trn_dataset[idx]\n",
    "    img = img.astype('uint8')\n",
    "    labelmap = imgviz.label2rgb(label=label_img, colormap=cmap)\n",
    "    labelviz = plot_rgb(img, label_img)\n",
    "\n",
    "    plt.figure(dpi=400)\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.text(1200, 50, 'Raw', **text_kwargs)\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(labelmap)\n",
    "    plt.text(1200, 50, 'Label', **text_kwargs)\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(labelviz)\n",
    "    plt.text(1200, 50, 'True', **text_kwargs)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Distribution\n",
    "\n",
    "To quantitatively evaluate the imbalance level of the dataset, we demonstrate the relative distribution by both pixel and image occurrence of each class. Here we state an important observation: **The class imbalance is highly dependent on the stuff vs things frequency gap**, that the 5 stuff classes composite the absolute majority with regard to proportion of pixels, while things all have ratios under 0.5%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frqn_ins = np.zeros((12), dtype=int)        # Frequency by instances\n",
    "frqn_pxl = np.zeros((12), dtype=np.float64) # Frequency by pixels\n",
    "frqn_pos = np.zeros((720, 1280, 12))        # Frequency in position\n",
    "frqv_ins = np.zeros((12), dtype=int)\n",
    "frqv_pxl = np.zeros((12), dtype=np.float64)\n",
    "frqv_pos = np.zeros((720, 1280, 12)) \n",
    "\n",
    "if issave:\n",
    "    mat = np.load('../saved_models/frqn.npz')\n",
    "    frqn_ins, frqn_pxl, frqn_pos = mat['arr_0'], mat['arr_1'], mat['arr_2']\n",
    "    mat = np.load('../saved_models/frqv.npz')\n",
    "    frqv_ins, frqv_pxl, frqv_pos = mat['arr_0'], mat['arr_1'], mat['arr_2']\n",
    "else:\n",
    "    for batch_i, (x, y) in enumerate(tqdm(trn_dataloader)):\n",
    "        y = torch.Tensor(y).long()\n",
    "        frq = F.one_hot(y, num_classes=12)\n",
    "        frq = torch.sum(frq, axis=0)\n",
    "        for c in range(12):\n",
    "            s = torch.sum(frq[:,:,c]).numpy()\n",
    "            if s > 0:\n",
    "                frqn_ins[c] += 1\n",
    "                frqn_pxl[c] += s\n",
    "        frqn_pos += frq.numpy()\n",
    "    np.savez('../saved_models/frqn.npz', frqn_ins, frqn_pxl, frqn_pos)\n",
    "    for batch_i, (x, y) in enumerate(tqdm(cln_dataloader)):\n",
    "        y = torch.Tensor(y).long()\n",
    "        frq = F.one_hot(y, num_classes=12)\n",
    "        frq = torch.sum(frq, axis=0)\n",
    "        for c in range(12):\n",
    "            s = torch.sum(frq[:,:,c]).numpy()\n",
    "            if s > 0:\n",
    "                frqv_ins[c] += 1\n",
    "                frqv_pxl[c] += s\n",
    "        frqv_pos += frq.numpy()\n",
    "    np.savez('../saved_models/frqv.npz', frqv_ins, frqv_pxl, frqv_pos)\n",
    "\n",
    "rtn_ins = frqn_ins / len(trn_dataset)       # Ratio by instances on train\n",
    "rtn_pxl = frqn_pxl / np.sum(frqn_pxl)       # Ratio by pixels on train\n",
    "rtv_ins = frqv_ins / len(cln_dataset)       # Ratio by instances on validation\n",
    "rtv_pxl = frqv_pxl / np.sum(frqv_pxl)       # Ratio by pixels on validation\n",
    "rt_pxl = (frqn_pxl + frqv_pxl) / np.sum(frqn_pxl + frqv_pxl)\n",
    "frq_pos = frqn_pos + frqv_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_pxl = plt.subplots(figsize=(9, 4))\n",
    "cm = plt.cm.tab20(np.arange(0, 20))\n",
    "ax_ins = ax_pxl.twinx()\n",
    "\n",
    "width = 0.2\n",
    "x = np.array([np.where(np.argsort(rt_pxl)[::-1] == i) for i in range(12)]).reshape(-1)\n",
    "p1 = ax_pxl.bar(x-width,     rtn_pxl, width=width, color=cm[0], align='center', label='train pixel')\n",
    "p2 = ax_ins.bar(x,           rtn_ins, width=width, color=cm[1], align='center', label='train image')\n",
    "p3 = ax_pxl.bar(x+(width*1), rtv_pxl, width=width, color=cm[8], align='center', label='val pixel')\n",
    "p4 = ax_ins.bar(x+(width*2), rtv_ins, width=width, color=cm[9], align='center', label='val image')\n",
    "\n",
    "lst = [p1,p2,p3,p4]\n",
    "ax_pxl.legend(handles=lst, ncol=2, loc=0,\n",
    "            columnspacing=0.9, handlelength=1.4, handletextpad=0.5,)\n",
    "ax_pxl.set_xticks(x, labs, fontsize=10)\n",
    "ax_pxl.set_xlim([-0.5, 11.8])\n",
    "ax_pxl.set(ylim=[0, 0.5], ylabel='Ratio by Pixel', xlabel='Class')\n",
    "ax_ins.set(ylim=[0, 1.05], ylabel='Ratio by Image')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Image \n",
    "\n",
    "Despite the analysis from the frequency level, we further investigate the statistics of classes in the image space. To characterize the spacial distribution of things in the image, we calculate the average image per thing class as the heatmap. Compared with average images of other tasks such as road segmentation which show distinct patterns, it is hard to discover any specific hierarchies in the figure. We conclude **that aerial images are in lack of prior knowledge of thing classes**, which is relevant with the diverse viewpoints and scenes of UAVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgimg_class(lst):\n",
    "    nclass = len(lst)\n",
    "    frq_thg = frq_pos[:, :, lst]\n",
    "    rtn_thg = frq_thg / (len(trn_dataset) + len(cln_dataset))\n",
    "    rtn_max = np.sum(rtn_thg, axis=2).max()\n",
    "    rtn_thg = rtn_thg / rtn_max\n",
    "    img_thg = np.zeros((720, 1280, 3), dtype=np.float64) \n",
    "    for c in range(nclass):\n",
    "        img_thg += rtn_thg[:,:,c][:, :, None] * cmap[lst[c]]  \n",
    "    lab_thg = np.ones((720, 1280), dtype=int)\n",
    "    lab_thg[0, :nclass] = lst\n",
    "    return img_thg, lab_thg, rtn_max\n",
    "\n",
    "img_thg, lab_thg, rtn_max = avgimg_class(np.arange(1, 8))\n",
    "labelviz = imgviz.label2rgb(\n",
    "            label=lab_thg, image=img_thg.astype('uint8'), alpha=0.0,\n",
    "            label_names=labs, colormap=cmap, font_size=30, loc=\"rb\")\n",
    "\n",
    "bmap = mpl.cm.copper\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=rtn_max)\n",
    "fig, ax = plt.subplots(dpi=200)\n",
    "img = plt.imshow(labelviz)\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=bmap), \n",
    "             ax=ax, fraction=0.046*720/1280, pad=0.04, label='Ratio')\n",
    "ax.axis(\"off\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then evaluate the five loss functions on the AeroScapes dataset with three popular segmentation models. The models we use are UNet, DeepLabV3, and DeepLabV3+, all are commonly-used mainstream convolutional segmentation models. We use the five losses to train the models and evaluate the performance by mean Intersection Over Union score. It is notable that all classes contribute to the average score of mIoU. So minority and majority classes are equally important regardless of their pixel number distribution. \n",
    "\n",
    "### Training Curves\n",
    "\n",
    "Since the model training is time-consuming, we here directly load the trained model weights for evaluation. We use DeepLabV3+ trained with Tversky loss as the representative. First we plot the curves during training epochs. It can be seen that the model reaches a highest mIoU of 67%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dual(los_trn, los_val, acc_val):\n",
    "    fig, ax_acc = plt.subplots(figsize=(8, 5))\n",
    "    cm = plt.cm.tab20(np.arange(0, 20))\n",
    "\n",
    "    cl_acc = cm[6]\n",
    "    l2 = ax_acc.plot(np.arange(1, 31), acc_val, '-', color=cl_acc, label='Val IoU')\n",
    "    ax_acc.set_xlabel('Train Epoch')\n",
    "    ax_acc.set_xlim([1, 30])\n",
    "    ax_acc.set_ylabel('Accuracy (\\%)', color=cl_acc)\n",
    "    ax_acc.set_ylim([0, 75])\n",
    "    ax_acc.tick_params(axis='y', labelcolor=cl_acc)\n",
    "    ax_acc.grid(True, ls='--')\n",
    "    # ax_omg.set_title('$\\\\theta$ and $\\omega$ over time')\n",
    "\n",
    "    ax_los = ax_acc.twinx()\n",
    "    cl_los = cm[0]\n",
    "    l3 = ax_los.plot(np.arange(1, 31), los_trn, '--', color=cl_los, label='Train Loss')\n",
    "    l4 = ax_los.plot(np.arange(1, 31), los_val, '-', color=cl_los, label='Val Loss')\n",
    "    ax_los.set_ylabel('Loss', color=cl_los)\n",
    "    ax_los.set_ylim([0, max(los_trn + los_val)])\n",
    "    ax_los.tick_params(axis='y', labelcolor=cl_los)\n",
    "    ax_los.grid(False)\n",
    "    \n",
    "    acc_b = max(acc_val)\n",
    "    epoch_b = acc_val.index(acc_b)\n",
    "    loss_b = los_val[epoch_b]\n",
    "    ax_acc.plot([epoch_b, epoch_b], [-5, 105], ':', color=cm[14])\n",
    "    ax_acc.plot([epoch_b], [acc_b], 'o', color=cl_acc)\n",
    "    ax_los.plot([epoch_b], [loss_b], 'o', color=cl_los)\n",
    "    textstr = '\\n'.join((\n",
    "        r'Epoch: %d' % (epoch_b, ),\n",
    "        r'IoU: %.2f' % (acc_b, ),\n",
    "        r'Loss: %.2f' % (loss_b, )))\n",
    "    props = dict(boxstyle='square', ec=[0.8]*3, facecolor='white')\n",
    "    if epoch_b/40 < 0.8:\n",
    "        x_txt = epoch_b/40 + 0.03\n",
    "    else:\n",
    "        x_txt = epoch_b/40 - 0.18\n",
    "    ax_acc.text(x_txt, 0.58, textstr, transform=ax_acc.transAxes,\n",
    "        verticalalignment='center', bbox=props, fontsize=12)\n",
    "    \n",
    "    lst = l2 + l3 + l4\n",
    "    lbst = [l.get_label() for l in lst]\n",
    "    ax_acc.legend(lst, lbst, ncol=1, \n",
    "                  columnspacing=0.9, handlelength=1.4, handletextpad=0.5,\n",
    "                  fontsize=12, loc=4)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def plot_dual_call(dir_save, dir_flag):\n",
    "    file_log = os.path.join(dir_save, dir_flag, 'log.txt')\n",
    "    with open(file_log, 'r') as f:\n",
    "        s = f.read()\n",
    "\n",
    "    it_tra = re.finditer(\n",
    "        r' trn loss:(\\d+\\.\\d+), trn acc:', s)\n",
    "    loss_tra = []\n",
    "    for i in it_tra:\n",
    "        loss_tra.append(float(i.group(1)))\n",
    "\n",
    "    it_val = re.finditer(\n",
    "        r' val loss:(\\d+\\.\\d+), val iou:(\\d+\\.\\d+)', s)\n",
    "    loss_val, acc_val = [], []\n",
    "    for i in it_val:\n",
    "        loss_val.append(float(i.group(1)))\n",
    "        acc_val.append(100 * float(i.group(2)))\n",
    "    \n",
    "    fig = plot_dual(loss_tra[:30], loss_val[:30], acc_val[:30])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_save = '../saved_models/'\n",
    "dir_flag = 'deeplabv3plus/tversky_0.70_0.30_1.00_1118'\n",
    "fig = plot_dual_call(dir_save, dir_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-wise IoU \n",
    "\n",
    "To further investigate how the loss guides the model to learn each class, we track the class-wise IoU during training. It indicates that the criterion has difficulties in recognizing minor classes, especially things with great variety such as animal and obstacle, while some minority classes with distinct characteristics such as car and boat are easier to learn. \n",
    "Typically, Tversky losses put more effort in learning minority classes, and their segmentation performances are less entangled with the imbalanced class size. It converges faster on minority labels during training and achieves better accuracy in classes drone and animal. Hence Tversky loss is powerful in balancing learning among classes and achieves the best mIoU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class(iou_lst):\n",
    "    iou_mat = np.array(iou_lst).T\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    img = plt.imshow(iou_mat, interpolation='nearest', \n",
    "                    cmap=plt.cm.coolwarm_r, vmin=0.0, vmax=1.0)\n",
    "    # set labels\n",
    "    n_cls, n_epoch = iou_mat.shape\n",
    "    fig.colorbar(img, ax=ax, fraction=0.046*n_cls/n_epoch, pad=0.04)\n",
    "    labs = ['bckgrnd', 'person', 'bike', 'car', 'drone', 'boat', 'animal', 'obstacle', 'cnstn', 'plant', 'road', 'sky']\n",
    "    ax.set(yticks=np.arange(n_cls), xticks=np.arange(4, 31, 5),\n",
    "            yticklabels=labs, xticklabels=np.arange(5, 31, 5),\n",
    "            ylabel='Val IoU by Class', xlabel='Train Epoch')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def plot_class_call(dir_flag, sav_flag):\n",
    "    file_log = os.path.join('../saved_models/', dir_flag, 'log.txt')\n",
    "    with open(file_log, 'r') as f:\n",
    "        s = f.read()\n",
    "\n",
    "    it_epoch = re.finditer(\n",
    "        r'iou class:' + ','.join([r'(\\d+\\.\\d+)' for _ in range(12)]), s)\n",
    "    iou_lst = []\n",
    "    for e in it_epoch:\n",
    "        iou_lst.append([float(e.group(c)) for c in range(1, 13)])\n",
    "\n",
    "    fig = plot_class(iou_lst)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_class_call('deeplabv3plus/tversky_0.70_0.30_1.00_1118', 'iou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform particular analysis on how the segmentation perform among the imbalanced classes when trained with different loss functions. We mainly look into the statistics of false negatives and false positives in this section.\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "Firstly, we plot the confusion matrix of the model trained with Tversky loss. Its diagonal is the correctly classified pixels, which is inline with the previous plot. \n",
    "It is significant to observe the false negative effect in imbalanced learning, that the model tends to regard minority thing pixels as more frequent stuff classes, as they are more possible to appear in the data distribution. Hence the entries of stuff columns above the diagonal are relatively high. \n",
    "\n",
    "Our conclusion is that, **Tversky is good at mitigate false negative predictions**. It achieves better performance on things especially those with lowest IoUs including bike, animal and obstacle. It is as well effective in reducing the false positive background pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mname = 'deeplabv3plus'\n",
    "flag_run = \"tversky_0.70_0.30_1.00_1118\"\n",
    "logger = Logger(save_path='../saved_models/', prj_name=mname, flag_run=flag_run)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def restore_model(mname, logger):\n",
    "    assert logger.path_existed, f\"Path {logger.dir_save} not found\"\n",
    "    model_logger = ModelLogger(logger, state_only=True)\n",
    "    model_logger.metric_name = 'iou'\n",
    "    model = get_model(mname)\n",
    "    model = model_logger.load_model('best', model=model).to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = restore_model(mname, logger)\n",
    "if issave:\n",
    "    cfms = np.load(logger.path_join('conf_mat.npy'))\n",
    "else:\n",
    "    cfms = np.zeros((12, 12), dtype=np.float64)\n",
    "    for batch_i, (xy_val, xy_cln) in enumerate(zip(val_dataloader, cln_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            xs, _ = xy_val\n",
    "            pred_mask = model(xs.to(device))\n",
    "        pred_mask = torch.softmax(pred_mask, dim=1)\n",
    "        y_pred = torch.argmax(pred_mask, dim=1)\n",
    "        y_pred = y_pred.cpu()\n",
    "        # transfer to label map\n",
    "        h, w = y_pred.shape[1:]\n",
    "        y_pred = transforms.functional.crop(y_pred, 8, 0, h-16, w)\n",
    "        # get true label\n",
    "        _, y_true = xy_cln\n",
    "\n",
    "        y_pred = y_pred.reshape(-1).numpy()\n",
    "        y_true = y_true.reshape(-1).numpy()\n",
    "        cfm = sklearn.metrics.confusion_matrix(y_true, y_pred, labels=range(12))\n",
    "        cfms += cfm\n",
    "    np.save(logger.path_join('conf_mat.npy'), cfms)\n",
    "\n",
    "def plot_confmat(cfm_rn):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    img = plt.imshow(cfm_rn, interpolation='nearest', \n",
    "                    cmap=plt.cm.Blues, vmin=0.0, vmax=1.0)\n",
    "    # set labels\n",
    "    n_cls = cfm_rn.shape[0]\n",
    "    fig.colorbar(img, ax=ax, fraction=0.046, pad=0.04)\n",
    "    labs = ['bckgrnd', 'person', 'bike', 'car', 'drone', 'boat', 'animal', 'obstacle', 'constrn', 'plant', 'road', 'sky']\n",
    "    ax.set(yticks=np.arange(n_cls), yticklabels=labs,\n",
    "            ylabel='True Label', xlabel='Predicted Label')\n",
    "    ax.set_xticks(np.arange(n_cls), labs, rotation='vertical')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm_rn = cfms / cfms.sum(axis=1, keepdims=True)\n",
    "fig = plot_confmat(cfm_rn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sankey Plot\n",
    "\n",
    "The confusion matrix is only concerned with the statistical IoU among classes, and cannot dive into the pixel level. Hence, we draw the Sankey plot as an approach to study how pixels in the aerial images are classified by the segmentation model. It shows the absolute value distribution of pixels misclassified to each class by the model. \n",
    "It can be observed that classes with more complex patterns such as obstacle, construction and plant are more likely to be misclassified, while labels of less distinct patterns including road and sky tend to receive more wrong predictions. \n",
    "the advantages of Tversky loss is represented by that it **reduces the number of misclassified pixels as things in ground truth**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row, col = [], []\n",
    "cfm_err = np.zeros(12*11)\n",
    "for r in range(12):\n",
    "    for c in range(12):\n",
    "        if r != c:\n",
    "            row.append(labs[r])\n",
    "            col.append(labs[c])\n",
    "            cfm_err[r*11+c] = cfms[r, c]\n",
    "cdict = {labs[i]: np.append(cmap[i] / 255, 1.) for i in range(12)}\n",
    "cdict['bckgrnd'] = np.array((128/255, 128/255, 128/255, 1.))\n",
    "labbak  = ['person', 'bike', 'car', 'drone', 'boat', 'animal', 'obstacle', 'constrn', 'plant', 'road', 'sky', 'bckgrnd']\n",
    "fig = sankey(left=row, right=col, leftWeight=cfm_err, colorDict=cdict, aspect=10,\n",
    "             leftLabels=list(reversed(labbak)), rightLabels=list(reversed(labbak)), fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Cases\n",
    "\n",
    "Finally we conduct case studies. For good cases, we state that region-based losses are better in deciding the boundaries of classes while identifying small things in the image. **Tversky loss is particularly good in figuring out small things**, such as the obstacle-person-bike triplet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_case(idx):\n",
    "    # get image\n",
    "    xs, ys = [], []\n",
    "    for i in idx:\n",
    "        x, y = val_dataset[i]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    xs = torch.stack(xs)\n",
    "    ys = torch.stack(ys)\n",
    "    # get prediction\n",
    "    with torch.no_grad():\n",
    "        pred_mask = model(xs.to(device))\n",
    "    pred_mask = torch.softmax(pred_mask, dim=1)\n",
    "    pred = torch.argmax(pred_mask, dim=1)\n",
    "    pred = pred.cpu()\n",
    "    # transfer to label map\n",
    "    h, w = pred.shape[1:]\n",
    "    pred = transforms.functional.crop(pred, 8, 0, h-16, w)\n",
    "\n",
    "    for batch_i, i in enumerate(idx):\n",
    "        img, label_img = cln_dataset[i]\n",
    "        img = img.astype('uint8')\n",
    "        labelviz = plot_rgb(img, label_img)\n",
    "        label_pred = pred[batch_i].numpy()\n",
    "        predviz = plot_rgb(img, label_pred)\n",
    "\n",
    "        plt.figure(dpi=400)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(labelviz)\n",
    "        plt.text(1200, 50, 'True', **text_kwargs)\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(predviz)\n",
    "        plt.text(1200, 50, 'Predict', **text_kwargs)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_case([300, 400, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad Cases\n",
    "\n",
    "There are also some cases where loss functions fail, mostly images with complicated semantics or bad ground truth labeling. For example, in the first row, the loss cannot distinguish the border between road, plant, and background. It also generate false positive class as well due to the noisy patterns in the image. \n",
    "In the second example, as the scene is too complex to understand, it fails in identifying the entire structure of animal and obstacle objects in the image. Additionally, it presents certain levels of false positive that expand plant pixels to the background. We suggest this may be caused by the green color of water background in the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_case([100, 200, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As conclusion, we state the pros and cons of our method: \n",
    "\n",
    "* To the best of our knowledge, we are the first work of introducing various loss functions to particularly address the class imbalance problem in aerial image dataset. Compared to solutions such as ensemble learning, our approach does not require additional efforts on specialized model structure, while being simple and effective. It can also be easily generalized and integrated with other methods. \n",
    "\n",
    "* We perform comprehensive analysis on dataset, loss functions, and the experimental results. We provide insightful explanations on why Tversky loss performs the best in mIoU. \n",
    "\n",
    "* In the future, more carefully-designed loss functions can be explored, and model architectural improvements can be introduced, to further improve performance on imbalanced datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b26e4677d92ed6aed28898b610174e7e899be7c998b12f1bc50b84aae23a360c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
